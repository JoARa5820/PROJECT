{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 코드 사용법 :\n",
    "# 1. 해당 코드 내에서는 이상값에 대한 처리를 진행하지 않음\n",
    "# 2. 사용 파일의 형식은 'csv'를 기준으로 진행\n",
    "# 3. 'Transform_Year_data.ipynb' 코드 수행으로 생성된 데이터셋을 이용한다고 가정하므로 기본적으로 ['YEAR','target'] 컬럼은 존재해야함\n",
    "#    - 연도 컬럼명은 'YEAR'\n",
    "#    - 타겟 컬럼명은 'target'\n",
    "\n",
    "## 코드 내에서 진행되는 독립변수의 전처리 및 변수 선택 과정\n",
    "# (1) 독립변수에 대해 표준화\n",
    "# - StandardScaler : 평균은 0, 표준편차는 1로 변환\n",
    "# - Normalization : 최소값을 0, 최대값을 1로 변환 (0~1 사이의 값으로 변환) : 이상값에 영향 받으므로 이상값 처리 후 사용\n",
    "# - RobustScaler : 중앙값을 0, IQR(1분위수~3분위수)을 1로 변환 : 이상값의 영향을 최소화 시키는 표준화 방법\n",
    "# (2) 상관분석\n",
    "# - 상관계수 값의 기준을 본인이 설정 (y와 x변수간의 단일 상관계수 값이 'corr_stand_val' 이상인 x변수만 입력변수로 사용)\n",
    "# (3) Null 값이 포함된 변수 제외\n",
    "# - 특정 연도의 값이 Null 값인 경우, 해당 변수를 입력변수 목록에서 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AutoML 설명\n",
    "# (1) AutoML은 총 20여 개의 머신러닝 및 딥러닝 모델을 포함한 알고리즘임\n",
    "# (2) 학습 시 해당 데이터에 가장 적합한 모델을 선택하며, 파라미터 또한 최적화 된 값을 선택해줌\n",
    "# (3) AutoML은 머신러닝 기반의 모형이기 때문에, 통계적 모형인 회귀 모형과는 달리 다중공선성 등의 과정을 진행하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## 0. 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 라이브러리 호출 #####\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 데이터프레임 출력 옵션\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "#지수표현\n",
    "pd.options.display.float_format = '{:.5f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## 1. 입력값 기입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 기본 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_folder 속 csv 파일 목록 호출 (파일 목록 확인)\n",
    "# data_folder = 'data/transform'\n",
    "# file_list = glob.glob(data_folder + '/' + '*.csv')\n",
    "# file_list = list(pd.Series(file_list).map(lambda x : x.split('/')[2].split('.')[0]))\n",
    "# file_list.sort()\n",
    "# file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_folder : 원본데이터 위치(폴더명)\n",
    "## save_folder : 결과 저장 위치(폴더명)\n",
    "## file_nm : 파일명\n",
    "## y_colnm : y 컬럼명\n",
    "## test_year : test년도 (리스트 형식으로 여러값 넣어줘도 됨 : ex. ['2020','2021'])\n",
    "\n",
    "# data_folder = 'data/transform'\n",
    "# save_folder = 'result'\n",
    "# file_nm = ['month_merge_cls_new', 'quarter_merge_cls_new', 'half_merge_cls_new', 'year_merge_cls_new']\n",
    "# y_colnm = ['SEP_CNT','SEP_CNT','SEP_CNT','SEP_CNT']\n",
    "# test_year = ['2021','2021','2021','2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (임시) 예시 - 삭제 가능\n",
    "data_folder = 'data/transform'\n",
    "save_folder = 'result'\n",
    "file_nm = ['month_merge_cls_new', 'quarter_merge_cls_new', 'half_merge_cls_new', 'year_merge_cls_new']\n",
    "y_colnm = ['SEP_CNT','SEP_CNT','SEP_CNT','SEP_CNT']\n",
    "test_year = ['2021','2021','2021','2021']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 표준화 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 독립변수(x)에 대하여 표준화 진행 시 원하는 표준화 기법 선택 (입력 안하면 표준화 진행 X)\n",
    "# 1 : 표준화1(StandardScaler) : 평균 = 0 / 표준편차 = 1\n",
    "# 2 : 표준화2(Normalization) : MinMaxScaler : 최소값 0 ~ 최대값 1 : 이상값에 영향 받음\n",
    "# 3 : 표준화3(RobustScaler) : 중앙값 = 0 / IQR(1분위(25%) ~ 3분위(75%)) = 1 \n",
    "select_scaler = 1  # (1, 2, 3) 중 택 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 상관분석 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 상관계수 값 설정 (y와 x변수간의 단일 상관계수 값이 'corr_stand_val' 이상인 x변수만 입력변수로 사용)\n",
    "# 입력변수 : 모델링에 이용되는 변수\n",
    "corr_stand_val = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## 2. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 정보 통합\n",
    "file_info = pd.DataFrame(file_nm).rename(columns = {0:'file_nm'})\n",
    "file_info['y_colnm'] = y_colnm\n",
    "file_info['test_year'] = test_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2o 호출\n",
    "h2o.init(nthreads=1)\n",
    "\n",
    "for file_num in range(len(file_info)):\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------ #\n",
    "    # 분석 정보 호출\n",
    "    file_nm = file_info['file_nm'][file_num]\n",
    "    y_colnm = file_info['y_colnm'][file_num]\n",
    "    test_year = file_info['test_year'][file_num]\n",
    "\n",
    "    # 데이터 호출\n",
    "    tot_data = pd.read_csv(data_folder + '/' + file_nm + '.csv', dtype='str', encoding = 'utf-8')\n",
    "    \n",
    "    # train 연도 정의 (위에서 정의한 test년도 제외)\n",
    "    train_year = list(tot_data['YEAR'].unique())\n",
    "    if isinstance(test_year, str):  # test_year가 str 형식일 때 : 값이 1개일 때\n",
    "        train_year.remove(test_year)\n",
    "    else:  # test_year가 list 형식일 때 : 값이 여러개일 때\n",
    "        train_year = list(set(train_year).difference(set(test_year)))\n",
    "    # ------------------------------------------------------------------------------------------------------------------ #\n",
    "    ## 데이터 구분\n",
    "    # 월 데이터\n",
    "    if file_nm.find('month') != -1:\n",
    "        tot_data['STAND_TIME'] = tot_data['YEAR'] + tot_data['MONTH']\n",
    "        except_colnm = ['YEAR','MONTH','STAND_TIME']\n",
    "    # 분기 데이터\n",
    "    elif file_nm.find('quarter') != -1:\n",
    "        tot_data['STAND_TIME'] = tot_data['YEAR'] + tot_data['QUARTER']\n",
    "        except_colnm = ['YEAR','QUARTER','STAND_TIME']\n",
    "    # 반기 데이터\n",
    "    elif file_nm.find('half') != -1:\n",
    "        tot_data['STAND_TIME'] = tot_data['YEAR'] + tot_data['HALF']\n",
    "        except_colnm = ['YEAR','HALF','STAND_TIME']\n",
    "    # 연 데이터\n",
    "    else:\n",
    "        tot_data['STAND_TIME'] = tot_data['YEAR']\n",
    "        except_colnm = ['YEAR','STAND_TIME']\n",
    "    # ------------------------------------------------------------------------------------------------------------------ #\n",
    "    # 독립변수 컬럼명 정의 : x_colnm\n",
    "    tot_colnm = list(tot_data.columns)                                 # 전체 컬럼명\n",
    "    except_colnm = ([y_colnm] + ['target'] + except_colnm)             # 독립변수에서 제외할 컬럼명\n",
    "    x_colnm = list(set(tot_colnm).difference(set(except_colnm)))       # 독립변수 컬럼명\n",
    "\n",
    "    # tot_data 컬럼 정리\n",
    "    tot_data = tot_data[['STAND_TIME','YEAR','target'] + [y_colnm] + x_colnm]\n",
    "    # ------------------------------------------------------------------------------------------------------------------ #\n",
    "    ## 모델 생성\n",
    "    \n",
    "    # loop_target : 타겟 리스트 : 소단위 모델 생성\n",
    "    loop_target = list(set(tot_data['target']))\n",
    "    \n",
    "    for trg in loop_target:\n",
    "\n",
    "        print('file_nm :',file_nm,' / trg :',trg)\n",
    "\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        # 모델 생성 데이터 정의 : 소단위 모델\n",
    "        data = tot_data.loc[tot_data['target'] == trg,].sort_values(by = 'STAND_TIME').reset_index(drop=True)\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        # 데이터 형 변환(str -> float)\n",
    "        data[[y_colnm] + x_colnm] = data[[y_colnm] + x_colnm].astype('float')\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        # (train / test) 데이터 분할\n",
    "        train = data.loc[data['YEAR'].isin(train_year),[y_colnm] + x_colnm]\n",
    "        train.reset_index(drop=True, inplace=True)\n",
    "        test = data.loc[~data['YEAR'].isin(train_year),[y_colnm] + x_colnm]\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # (train_x / train_y / test_x / test_y) 데이터 분할\n",
    "        train_x = train[x_colnm]\n",
    "        train_y = train[[y_colnm]]\n",
    "        test_x = test[x_colnm]\n",
    "        test_y = test[[y_colnm]]\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        ## 표준화 수행\n",
    "        \n",
    "        # StandardScaler : 평균 0, 표준편차 1\n",
    "        if select_scaler == 1:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler = StandardScaler()   \n",
    "            mody_train_x = pd.DataFrame(scaler.fit_transform(train_x), columns = list(train_x.columns))\n",
    "            mody_test_x = pd.DataFrame(scaler.transform(test_x), columns = list(test_x.columns))\n",
    "        \n",
    "        # Normalization : MinMaxScaler : 최소값 0 ~ 최대값 1\n",
    "        elif select_scaler == 2:\n",
    "            from sklearn.preprocessing import MinMaxScaler\n",
    "            scaler = MinMaxScaler()\n",
    "            mody_train_x = pd.DataFrame(scaler.fit_transform(train_x), columns = list(train_x.columns))\n",
    "            mody_test_x = pd.DataFrame(scaler.transform(test_x), columns = list(test_x.columns))\n",
    "\n",
    "        # RobustScaler : 중앙값 0, IQR(1분위(25%) ~ 3분위(75%)) 1 : 이상치(outlier) 영향 최소화 / 더 넓게 분포\n",
    "        elif select_scaler == 3:\n",
    "            from sklearn.preprocessing import RobustScaler\n",
    "            scaler = RobustScaler()\n",
    "            mody_train_x = pd.DataFrame(scaler.fit_transform(train_x), columns = list(train_x.columns))\n",
    "            mody_test_x = pd.DataFrame(scaler.transform(test_x), columns = list(test_x.columns))\n",
    "        \n",
    "        # 표준화 수행 X\n",
    "        else:\n",
    "            mody_train_x = train_x\n",
    "            mody_test_x = test_x\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        # inf / -inf 값을 null 처리\n",
    "        mody_train_x = mody_train_x.replace([np.inf, -np.inf], np.nan)\n",
    "        mody_test_x = mody_test_x.replace([np.inf, -np.inf], np.nan)        \n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        # 모델에 사용할 train, test 데이터셋\n",
    "        mdl_train_data = pd.concat([train_y, mody_train_x], axis = 1)\n",
    "        mdl_test_data = mody_test_x\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        ## 독립변수 선택\n",
    "\n",
    "        # 1. 상관분석 : 상관분석은 train 데이터셋에 대해서만 진행 (test 데이터셋 이용 X)\n",
    "        corr = mdl_train_data.corr(method = 'pearson')  # default는 'pearson'\n",
    "        corr = corr.reset_index().rename(columns = {'index':'COLNM'})\n",
    "        corr = corr.loc[corr['COLNM'] != y_colnm,]\n",
    "        corr = corr[corr[y_colnm] >= corr_stand_val]\n",
    "\n",
    "        # 2. 상관분석 결과로 선택된 독립변수 목록\n",
    "        mdl_x_colnm = list(corr['COLNM'])\n",
    "\n",
    "        # 3. 독립변수 목록 중 null값이 없는 독립변수만 선택\n",
    "        train_na_col = []\n",
    "        for col in mdl_test_data.columns:\n",
    "            if len(mdl_train_data.loc[mdl_train_data[col].isna(),]) != 0:\n",
    "                train_na_col.append(col)\n",
    "\n",
    "        test_na_col = []\n",
    "        for col in mdl_test_data.columns:\n",
    "            if len(mdl_test_data.loc[mdl_test_data[col].isna(),]) != 0:\n",
    "                test_na_col.append(col)\n",
    "\n",
    "        tot_na_col = list(set(train_na_col + test_na_col))  # null값이 있는 독립변수들\n",
    "        mdl_x_colnm = list(set(mdl_x_colnm).difference(set(tot_na_col)))  # 모델에 사용할 독립변수들(상관분석 결과 - null값이 있는 변수)\n",
    "        \n",
    "        if len(mdl_x_colnm) == 0:  # 모델에 사용할 독립변수의 수가 0이면 => 기본 독립변수 중 null값이 없는 변수를 선택 \n",
    "            mdl_x_colnm = x_colnm\n",
    "            mdl_x_colnm = list(set(mdl_x_colnm).difference(set(tot_na_col)))  # 모델에 사용할 독립변수들\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        # h2o 데이터프레임 형식으로 변환\n",
    "        h2o_train_data = h2o.H2OFrame(mdl_train_data)\n",
    "        h2o_test_data = h2o.H2OFrame(mdl_test_data)\n",
    "\n",
    "        ## 모델 생성\n",
    "        model = H2OAutoML(max_models=20, max_runtime_secs=10, seed=1234)\n",
    "        model.train(x = mdl_x_colnm, y = y_colnm,\n",
    "                    training_frame = h2o_train_data)  # x : 독립변수 / y : 종속변수 / training_frame : 학습데이터 / 모델 검증은 pass\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        # # View the AutoML Leaderboard\n",
    "        # lb = model.leaderboard\n",
    "        # lb.head(rows = 10)  # 가장 성능 좋은 모델 top 10개 확인\n",
    "        # model.leader  # 리더보드 값 확인 : The leader model is stored here\n",
    "\n",
    "        # ## 모델 조사\n",
    "        # m = model.leader  # Get the best model using the metric\n",
    "        # m = model.get_best_model()  # this is equivalent to\n",
    "\n",
    "        ## AutoML 출력\n",
    "        # Get leaderboard with all possible columns\n",
    "        lb = h2o.automl.get_leaderboard(model, extra_columns = \"ALL\")  # lb : top 10개 모델에 대한 리더보드 확인\n",
    "        save_lb = lb.as_data_frame()  # pandas 데이터프레임으로 형변환\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        ## 예측 수행\n",
    "        pred = model.predict(h2o_test_data)\n",
    "\n",
    "        ## h2o 데이터프레임을 pandas 데이터프레임으로 변환\n",
    "        pred = h2o.as_list(pred, use_pandas=True)  # 또는 pred.as_data_frame()\n",
    "        pred.rename(columns={'predict':'PREDICT'}, inplace=True)\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        ## 결과값 정리\n",
    "        rslt = pd.concat([pred, test_y], axis = 1)\n",
    "        rslt['target'] = trg\n",
    "        rslt['stand_time'] = list(data.loc[~data['YEAR'].isin(train_year),'STAND_TIME'])\n",
    "        rslt['BEST_MDL'] = save_lb['model_id'][0]  # 최적 모델명\n",
    "        rslt['DIFF'] = rslt['PREDICT'] - rslt[y_colnm]\n",
    "        rslt['mdl_x_colnm'] = str(mdl_x_colnm)\n",
    "\n",
    "        rslt['PREDICT'] = round(rslt['PREDICT'],4)\n",
    "        rslt[y_colnm] = round(rslt[y_colnm],4)\n",
    "        rslt['DIFF'] = round(rslt['DIFF'],4)\n",
    "        \n",
    "        rslt = rslt[['target', 'stand_time', 'PREDICT', y_colnm, 'DIFF', 'BEST_MDL']]\n",
    "        # -------------------------------------------------------------------------------------------------------------- #\n",
    "        ## 결과값 저장\n",
    "        if trg == loop_target[0]:\n",
    "            fin_result = rslt\n",
    "        else:\n",
    "            fin_result = fin_result.append(rslt)\n",
    "\n",
    "    fin_result.to_csv(save_folder + '/result_' + file_nm + '.csv', index=False, encoding = 'utf-8')\n",
    "\n",
    "# h2o 종료\n",
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('총 모델 생성 시간 : ', str((time.time() - total_start_time)/60)[:7]+' 분 소요')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 결과 해석\n",
    "# - AutoML을 이용하여 (월/분기/반기/연) 모형을 개발해 본 결과, (시도별*연령별 / 시도별*학급(학교)별 / 시도별*장애영역별) 모형 모두 MSE 값이 가장 작은 모델로 '연 모형'이 채택되었음\n",
    "# - AutoML은 20여 개의 모델이 포함된 알고리즘이므로, 각 단위 별로 20여 개의 모델을 이용하여 (월/분기/반기/연) 모형을 테스트 해봤다고 할 수 있음\n",
    "# - 따라서, 각 단위 별 연 MSE 값이 가장 낮은 모델로 '연 모형'이 가장 많이 선택되었으므로 해당 데이터를 이용한 모델링은 '연 모형'이 적합하다고 판단됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
